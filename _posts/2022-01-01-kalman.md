---
layout: post
title: Introducing Hyde
author:
  name: Théo Gigant
  link: https://github.com/giganttheo
use_math: true
category: ML
layout: default
---


This notebook and the code inside it was written by Théo GIGANT

# Imports


```python
%matplotlib inline
#%pip install --no-cache --no-deps --force-reinstall https://github.com/dstl/Stone-Soup/archive/master.zip#egg=stonesoup
%pip install stonesoup
```
```python
import matplotlib.pyplot as plt
import numpy as np
from stonesoup.types.groundtruth import GroundTruthPath, GroundTruthState
from stonesoup.models.transition.linear import CombinedLinearGaussianTransitionModel, \
                                               ConstantVelocity
from datetime import datetime, timedelta

```

# 1 - Implementation of Kalman filter

### Create simulator for individual target trajectories and measurements according to linear and Gaussian dynamics and observation models


```python
np.random.seed(42)
start_time = datetime.now()
```

The class `CombinedLinearGaussianTransistionModel` allows to create linear and Gaussian dynamics


```python
transition_model = CombinedLinearGaussianTransitionModel([ConstantVelocity(0.05),
                                                          ConstantVelocity(0.05)])
```


```python
truth = GroundTruthPath([GroundTruthState([0, 1, 0, 1], timestamp=start_time)])

for k in range(1, 21):
    truth.append(GroundTruthState(
        transition_model.function(truth[k-1], noise=True, time_interval=timedelta(seconds=1)),
        timestamp=start_time+timedelta(seconds=k)))
```


```python
fig = plt.figure(figsize=(10, 6))
ax = fig.add_subplot(1, 1, 1)
ax.set_xlabel("$x$")
ax.set_ylabel("$y$")
ax.axis('equal')
ax.plot([state.state_vector[0] for state in truth],
        [state.state_vector[2] for state in truth],
        linestyle="--")
```




    [<matplotlib.lines.Line2D at 0x7fb26b282240>]




![png]({{ site.url }}/assets/output_10_1.png)


### Generate observations



```python
from stonesoup.types.detection import Detection
from stonesoup.models.measurement.linear import LinearGaussian
```


```python
noise_covar1 = np.array([[10,0],[0,10]])
noise_covar2 = np.array([[5,0], [0,20]])

measurement_model1 = LinearGaussian(
    ndim_state=4,  # Number of state dimensions (position and velocity in 2D)
    mapping=(0, 2),  # Mapping measurement vector index to state index
    noise_covar=noise_covar1 # Covariance matrix for Gaussian PDF
    )
measurement_model2 = LinearGaussian(
    ndim_state=4,  # Number of state dimensions (position and velocity in 2D)
    mapping=(0, 2),  # Mapping measurement vector index to state index
    noise_covar=noise_covar2  # Covariance matrix for Gaussian PDF
    )
```


```python
measurements1 = []
measurements2 = []
for state in truth:
    measurement1 = measurement_model1.function(state, noise=True)
    measurements1.append(Detection(measurement1, timestamp=state.timestamp))
    measurement2 = measurement_model2.function(state, noise=True)
    measurements2.append(Detection(measurement2, timestamp=state.timestamp))
```


```python
ax.scatter([state.state_vector[0] for state in measurements1],
           [state.state_vector[1] for state in measurements1],
           color='b')
fig

ax.scatter([state.state_vector[0] for state in measurements2],
           [state.state_vector[1] for state in measurements2],
           color='yellow')
fig
```




![png]({{ site.url }}/assets/output_15_0.png)



On this figure, the observations for the model 1 (with the lowest noise) is represented in blue, while in yellow we represent the observations for model 2 (with a higher noise)

### Implement prediction and update equations for the mean and covariance of a Kalman filter

We create 2 Kalman filters, each one will receive the observations from a different measurement model.


```python
from stonesoup.predictor.kalman import KalmanPredictor
predictor1 = KalmanPredictor(transition_model)
predictor2 = KalmanPredictor(transition_model)

from stonesoup.updater.kalman import KalmanUpdater
updater1 = KalmanUpdater(measurement_model1)
updater2 = KalmanUpdater(measurement_model2)
```


```python
from stonesoup.types.state import GaussianState
prior1 = GaussianState([[0], [1], [0], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)
prior2 = GaussianState([[0], [1], [0], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)

```


```python
from stonesoup.types.hypothesis import SingleHypothesis
```

We update the mean and covariance of each Kalman filter by using the observations by observation models 1 & 2.


```python
from stonesoup.types.track import Track
track1 = Track()
track2 = Track()

for measurement in measurements1:
    prediction1 = predictor1.predict(prior1, timestamp=measurement.timestamp)
    hypothesis1 = SingleHypothesis(prediction1, measurement)  # Group a prediction and measurement
    post1 = updater1.update(hypothesis1)
    track1.append(post1)
    prior1 = track1[-1]

for measurement in measurements2:
    prediction2 = predictor2.predict(prior2, timestamp=measurement.timestamp)
    hypothesis2 = SingleHypothesis(prediction2, measurement)  # Group a prediction and measurement
    post2 = updater2.update(hypothesis2)
    track2.append(post2)
    prior2 = track2[-1]
```


### Estimate the trajectories of targets



```python
ax.plot([state.state_vector[0] for state in track1],
        [state.state_vector[2] for state in track1],
        marker=".")

ax.plot([state.state_vector[0] for state in track2],
        [state.state_vector[2] for state in track2],
        marker=".", color='g')

from matplotlib.patches import Ellipse

for state in track1: 
    w, v = np.linalg.eig(measurement_model1.matrix()@state.covar@measurement_model1.matrix().T)
    max_ind = np.argmax(w)
    min_ind = np.argmin(w)
    orient = np.arctan2(v[1,max_ind], v[0,max_ind])
    ellipse = Ellipse(xy=(state.state_vector[0], state.state_vector[2]),
                      width=2*np.sqrt(w[max_ind]), height=2*np.sqrt(w[min_ind]),
                      angle=np.rad2deg(orient),
                      alpha=0.2,
                      color='r')
    ax.add_artist(ellipse)
fig

for state in track2:
    w, v = np.linalg.eig(measurement_model2.matrix()@state.covar@measurement_model2.matrix().T)
    max_ind = np.argmax(w)
    min_ind = np.argmin(w)
    orient = np.arctan2(v[1,max_ind], v[0,max_ind])
    ellipse = Ellipse(xy=(state.state_vector[0], state.state_vector[2]),
                      width=2*np.sqrt(w[max_ind]), height=2*np.sqrt(w[min_ind]),
                      angle=np.rad2deg(orient),
                      alpha=0.2,
                      color='g')
    ax.add_artist(ellipse)
fig
```




![png]({{ site.url }}/assets/output_25_0.png)



In red is represented the estimate of trajectories with the blue observations.

In green, the estimate of trajectories with the yellow observations.

# 2 - Information measure development

These functions return some information measures for multivariate Gaussians, according to the exercices of _"A tutorial on measures of information with
application to target tracking"_.

**Shannon entropy**

In the case of $p \sim \mathcal{N}(\mu_p, \Sigma)$ of dimension $d$ :
$$H(p) = \frac{1}{2}\log((2\pi e)^d \det(\Sigma))$$

**Kullback-Leibler divergence**

For multivariate Gaussians $p \sim \mathcal{N}(\mu_0, \Sigma_0)$ and $q \sim \mathcal{N}(\mu_1, \Sigma_1)$ :
$$D_{KL}(p||q) = \frac{1}{2}(\mu_1 - \mu_0)^T \Sigma_1^{-1}(\mu_1 - \mu_0) + \frac{1}{2}(\text{tr}(\Sigma_1^{-1}\Sigma_0) - d + \log(\frac{|\Sigma_1|}{|\Sigma_0|}))$$

**Chernoff divergence**

For multivariate Gaussians $p \sim \mathcal{N}(\mu_1, \Sigma_1)$ and $q \sim \mathcal{N}(\mu_2, \Sigma_2)$ :

$$D^{(\alpha)}(p||q) = \frac{1}{2}\log(\frac{|\alpha\Sigma_1 + (1-\alpha)\Sigma_2|}{|\Sigma_1|^\alpha|\Sigma_2|^{1 - \alpha}}) + {\frac{\alpha(1 - \alpha)}{2}(\mu_1 - \mu_2)^T(\alpha\Sigma_1 + (1 - \alpha)\Sigma_2)^{-1}(\mu_1 - \mu_2)}$$

**Rényi divergence**

For the Rényi divergence, the following formula gives its value for multivariate Gaussians $p \sim \mathcal{N}(\mu_p, \Sigma_p)$ and $q \sim \mathcal{N}(\mu_q, \Sigma_q)$.

$$H_{\alpha}(p||q) = \frac{1}{2}(\mu_p - \mu_q)^T(\alpha \Sigma_q + (1 - \alpha)\Sigma_p)^{-1}(\mu_p - \mu_q) - {\frac{1}{2\alpha(\alpha - 1)} {\ln(\frac{|\alpha\Sigma_q + (1 - \alpha) \Sigma_p|}{|\Sigma_p|^{1 - \alpha}|\Sigma_q|^\alpha})}}$$


Experimentaly, the limit $\lim_{\alpha \to 1}H_{\alpha}(p\|\|q) \longrightarrow D_{KL}(p\|\|q)$ seems to stand.

**Squared Hellinger distance**

For the Squared Hellinger distance $H^2(p,q)$ we can compute it with the value of the Chernoff divergence $D^{(\alpha)}(p\|\|q)$ by using the Bhattacharyya distance defined by :

$$D_B(p,q) = -ln(BC(p,q))$$

Where $BC(p,q)$ is the Bhattacharyya coefficient, equal to the Chernoff $\alpha$ -divergence for $\alpha = \frac{1}{2}$ for multivariate Gaussians.


Then :
$$H^2(p,q) = 1 - BC(p,q) = 1 - e^{-D_B(p,q)} = 1 - e^{- D^{(\frac{1}{2})}(p||q)}$$

**Bregman divergence**

For multivariate Gaussians $z \sim \mathcal{N}(z, \Sigma)$ and $y \sim \mathcal{N}(y, \Sigma_y)$, the Bregman divergence $D(z\|\|y)$ is equal to :
$$D(z||y) = \frac{1}{2}(z - y)^T\Sigma^{-1}(z-y)$$


```python
from stonesoup.measures import Measure

class ShannonEntropy(Measure):
  def __call__(self, state1):
    Sigma = state1.covar
    d = Sigma.shape[0]
    return 1/2 * np.log(((2*np.pi * np.e)**d ) * np.linalg.det(Sigma))

class KLDivergence(Measure):
  def __call__(self, state1, state2):
    mu0 = state1.state_vector
    mu1 = state2.state_vector
    Sigma0 = state1.covar
    Sigma1 = state2.covar
    d = Sigma0.shape[0]
    Sigma1_inv = np.linalg.inv(Sigma1)
    return (1/2 * np.dot(np.dot((mu1 - mu0).T, Sigma1_inv),mu1 - mu0) \
          + 1/2 * (np.trace(np.dot(Sigma1_inv, Sigma0)) - d + np.log(np.linalg.det(Sigma1)/np.linalg.det(Sigma0)))).item()

class ChernoffDivergence(Measure):
  def __call__(self, state1, state2, alpha):
    mu1 = state1.state_vector
    mu2 = state2.state_vector
    Sigma1 = state1.covar
    Sigma2 = state2.covar
    return (1/2 * np.log((np.linalg.det(alpha * Sigma1 + (1-alpha) * Sigma2))/(np.linalg.det(Sigma1)**alpha * np.linalg.det(Sigma2)**(1-alpha))) \
          + (alpha * (1-alpha))/2 * np.dot(np.dot((mu1 - mu2).T, np.linalg.inv(alpha*Sigma1 + (1-alpha)*Sigma2)), (mu1 - mu2))).item()

class RenyiDivergence(Measure):
  def __call__(self, state1, state2, alpha):
    mu1 = state1.state_vector
    mu2 = state2.state_vector
    Sigma1 = state1.covar
    Sigma2 = state2.covar
    return (1/2 * (mu1 - mu2).T @ np.linalg.inv(alpha*Sigma2 + (1-alpha)*Sigma1) @ (mu1 - mu2)\
            - 1/(2*alpha * (alpha - 1)) * np.log((np.linalg.norm(alpha * Sigma2 + (1 - alpha) * Sigma1 )\
            / (np.linalg.norm(Sigma1)**(1-alpha) * np.linalg.norm(Sigma2)**alpha)))).item()

class SquaredHellingerDistance(Measure):
  def __call__(self, state1, state2):
    return 1 - np.exp(-chernoff_divergence(state1, state2, 1/2))

class BregmanDivergence(Measure):
  def __call__(self, state1, state2):
    z = state1.state_vector
    y = state2.state_vector
    Sigma1 = state1.covar
    Sigma2 = state2.covar
    return (1/2 * (z - y).T @ np.linalg.inv(Sigma1) @ (z - y)).item()
```


```python
alpha_chernoff = 0.2
alpha_renyi = 0.5

shannon_entropy = ShannonEntropy()
kl_divergence = KLDivergence()
chernoff_divergence = ChernoffDivergence()
renyi_divergence = RenyiDivergence()
squared_hellinger_distance = SquaredHellingerDistance()
bregman_divergence = BregmanDivergence()

Sh_entropy_1 = []
Sh_entropy_2 = []
KL_div = []
Ch_div = []
Re_div = []
Sq_He_dist = []
Br_div = []

for t_i, state1 in enumerate(track1):
  state2 = track2[t_i]
  # print(f"Shannon entropy for signal 1 at timestep {t_i} = {shannon_entropy(state1)}")
  Sh_entropy_1.append(shannon_entropy(state1))
  # print(f"Shannon entropy for signal 2 at timestep {t_i} = {shannon_entropy(state2)}")
  Sh_entropy_2.append(shannon_entropy(state2))
  # print(f"KL divergence at timestep {t_i} = {kl_divergence(state1, state2)}")
  KL_div.append(kl_divergence(state1, state2))
  # print(f"Chernoff divergence at timestep {t_i} (with alpha = {alpha_chernoff}) = {chernoff_divergence(state1, state2, alpha=alpha_chernoff)}")
  Ch_div.append(chernoff_divergence(state1,state2, alpha_chernoff))
  # print(f"Rényi divergence at timestep {t_i} (with alpha = {alpha_renyi}) = {renyi_divergence(state1, state2, alpha=alpha_renyi)}")
  Re_div.append(renyi_divergence(state1,state2,alpha_renyi))
  # print(f"Squared Hellinger distance at timestep {t_i} = {squared_hellinger_distance(state1, state2)}")
  Sq_He_dist.append(squared_hellinger_distance(state1, state2))
  # print(f"Bregman divergence at timestep {t_i} = {bregman_divergence(state1, state2)}")
  Br_div.append(bregman_divergence(state1,state2))
```


```python
entropy_diff = [np.abs(Sh_entropy_1[ti] - Sh_entropy_2[ti]) for ti in range(len(Sh_entropy_1))]


plt.plot(Sh_entropy_1, "r.-", label="Shannon entropy for Kalman filter with measurement model 1")
plt.plot(Sh_entropy_2, "b.-", label="Shannon entropy for Kalman filter with measurement model 2")
plt.title("Shannon entropy")
plt.legend()
plt.show()

plt.plot(KL_div, "-")
# plt.plot(entropy_diff, "yellow")
plt.title("KL divergence")
plt.show()

plt.plot(Ch_div, "-")
# plt.plot(entropy_diff, "yellow")
plt.title("Chernoff divergence")
plt.show()

plt.plot(Re_div, "-")
# plt.plot(entropy_diff, "yellow")
plt.title("Rényi divergence")
plt.show()

plt.plot(Sq_He_dist, "-")
# plt.plot(entropy_diff, "yellow")
plt.title("Squared Hellinger distance")
plt.show()

plt.plot(Br_div, "-")
# plt.plot(entropy_diff, "yellow")
plt.title("Bregman divergence")
plt.show()
```


![png]({{ site.url }}/assets/output_31_0.png)



![png]({{ site.url }}/assets/output_31_1.png)



![png]({{ site.url }}/assets/output_31_2.png)



![png]({{ site.url }}/assets/output_31_3.png)



![png]({{ site.url }}/assets/output_31_4.png)



![png]({{ site.url }}/assets/output_31_5.png)


The Shannon entropy depends only of the covariance of the Kalman filter, which depends of the noise variance. The entropy decreases with time because the model becomes better at each time stamp, and seems to tend to a limit.

We see that every divergence look almost the same. But with some light shape differences and different scales.


```python
for alpha in [0.1, 0.4, 0.6, 0.9]:
  Ch_div = []
  Re_div = []
  for t_i, state1 in enumerate(track1):
    state2 = track2[t_i]
    Ch_div.append(chernoff_divergence(state1, state2, alpha))
    Re_div.append(renyi_divergence(state1, state2, alpha))
  fig0, (ax1, ax2) = plt.subplots(1, 2)
  fig0.suptitle(f"Chernoff divergence (left) and Rényi divergence (right) for alpha = {alpha}")
  ax1.plot(Ch_div)
  ax2.plot(Re_div)
  plt.show()

```


![png]({{ site.url }}/assets/output_33_0.png)



![png]({{ site.url }}/assets/output_33_1.png)



![png]({{ site.url }}/assets/output_33_2.png)



![png]({{ site.url }}/assets/output_33_3.png)


Again, the divergences for different values of $\alpha$ are different in both shapes and scale.
It's difficult to interpret which one better suit assessing the performance of the filters, because each one gives a similar information.



```python
from scipy.optimize import fminbound
from numpy.linalg import inv, det
from scipy.stats import multivariate_normal

def covariance_intersection_algorithm(state1, state2):
  mean_a = state1.state_vector
  mean_b = state2.state_vector
  cov_a = state1.covar
  cov_b = state2.covar
  def optimize_fn(omega):
    return np.trace(inv(np.multiply(omega, inv(cov_a)) + np.multiply(1 - omega, inv(cov_b))))
  omega = fminbound(optimize_fn, 0, 1)
  covar = inv(np.multiply(omega, inv(cov_a)) + np.multiply(1 - omega, inv(cov_b)))
  state_vector = np.dot(covar, (np.dot(np.multiply(omega, inv(cov_a)), mean_a) + np.dot(np.multiply(1 - omega, inv(cov_b)), mean_b)))
  timestamp = state1.timestamp  
  return GaussianState(state_vector, covar, timestamp)


# x,y,z,t = np.arange(-2,2,0.2),np.arange(-2,2,0.2),np.arange(-2,2,0.2),np.arange(-2,2,0.2)
# SPACE = np.array([[xi,yi,zi,ti] for xi in x for yi in y for zi in z for ti in t])
# print(SPACE.shape)
# dV = 0.2**4


def chernoff_fusion_rule(state1, state2):
  #attempt to implement the fusion rule, as I understood it from "A Chernoff information fusion approach for distributed target tracking" Daniel E. Clark
  #this is not working for the moment
  pass
  # mean_a = state1.state_vector
  # mean_b = state2.state_vector
  # cov_a = state1.covar
  # cov_b = state2.covar
  # p_a = multivariate_normal.pdf(SPACE, np.array(mean_a).T[0], np.array(cov_a))
  # p_b = multivariate_normal.pdf(SPACE, np.array(mean_b).T[0], np.array(cov_b))
  # chernoff_divergence = ChernoffDivergence()
  # def optimize_fn(alpha):
  #   omega = chernoff_divergence(state1, state2, alpha)
  #   return - np.log(np.sum(p_a**omega * p_b ** (1 - omega) * dV))
  # alpha = fminbound(optimize_fn, 0, 1)
  # omega = chernoff_divergence(state1, state2, alpha)
  # D = - np.log(np.sum(p_a**omega * p_b ** (1 - omega)))
  # p_alpha = np.zeros(p_a.shape)
  # for tk in range(len(x)):
  #   p_a_k = p_a[tk]
  #   p_b_k = p_b[tk]
  #   p_alpha[tk] = ((p_a_k**omega)*(p_b_k**(1-omega))) / D
  # state_vector = esperance(p_alpha) #todo
  # covar = covariance(p_alpha) #todo
  # timestamp = state1.timestamp
  # return GaussianState(state_vector, covar, timestamp)
```


```python
Sh_entropy_1 = []
Sh_entropy_2 = []
Sh_entropy_ci = []
Sh_entropy_cf = []
track_ci = []
track_cf = []


for t_i, state1 in enumerate(track1):
  state2 = track2[t_i]
  state_ci = covariance_intersection_algorithm(state1, state2)
  track_ci.append(state_ci)
  # state_cf = chernoff_fusion_rule(state1, state2)
  # track_cf.append(state_cf)
  # print(f"Shannon entropy for signal 1 at timestep {t_i} = {shannon_entropy(state1)}")
  Sh_entropy_1.append(shannon_entropy(state1))
  # print(f"Shannon entropy for signal 2 at timestep {t_i} = {shannon_entropy(state2)}")
  Sh_entropy_2.append(shannon_entropy(state2))
  # print(f"Shannon entropy for signal fused at timestep {t_i} = {shannon_entropy(state3)}")
  Sh_entropy_ci.append(shannon_entropy(state_ci))
  # Sh_entropy_cf.append(shannon_entropy(state_cf))
```


```python
ax.plot([state.state_vector[0] for state in track_ci],
        [state.state_vector[2] for state in track_ci],
        marker=".", color="k")

fig
```




![png]({{ site.url }}/assets/output_37_0.png)



In this figure, the fused estimate is represented in black.


```python
# plt.plot([state.state_vector[0] for state in track_cf],
#          [state.state_vector[2] for state in track_cf],
#          ".-b")

plt.plot([state.state_vector[0] for state in truth],
         [state.state_vector[2] for state in truth],
         "--b", label="truth")

plt.plot([state.state_vector[0] for state in track2],
         [state.state_vector[2] for state in track2],
         ".-g", label="Kalman model 2")
plt.plot([state.state_vector[0] for state in track1],
         [state.state_vector[2] for state in track1],
         ".-r", label= "Kalman model 1")

plt.plot([state.state_vector[0] for state in track_ci],
         [state.state_vector[2] for state in track_ci],
         ".-k", label="fusion model 1+2")
plt.legend()
plt.show()
```


![png]({{ site.url }}/assets/output_39_0.png)


I plotted only the 3 estimates, alongside with the truth path.

## GM-PHD filter


I couldn't do this part, as the GM-PHD filter in `stonesoup.updater.*pointprocess*.PHDUpdater` is not yet implemented.


```python
from stonesoup.updater.pointprocess import PHDUpdater
```


```python
predictor_phd1 = KalmanPredictor(transition_model)
predictor_phd2 = KalmanPredictor(transition_model)

updater_phd1 = PHDUpdater(measurement_model1)
updater_phd2 = PHDUpdater(measurement_model2)
```


```python
prior_phd1 = GaussianState([[0], [1], [0], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)
prior_phd2 = GaussianState([[0], [1], [0], [1]], np.diag([1.5, 0.5, 1.5, 0.5]), timestamp=start_time)
```


```python
from stonesoup.types.track import Track
track_phd1 = Track()
for measurement in measurements1:
    prediction_phd1 = predictor_phd1.predict(prior_phd1, timestamp=measurement.timestamp)
    hypothesis_phd1 = SingleHypothesis(prediction_phd1, measurement)  # Group a prediction and measurement
    post_phd1 = updater_phd1.update(hypothesis_phd1)
    track_phd1.append(post_phd1)
    prior_phd1 = track_phd1[-1]

track_phd2 = Track()
for measurement in measurements2:
    prediction_phd2 = predictor_phd2.predict(prior_phd2, timestamp=measurement.timestamp)
    hypothesis_phd2 = SingleHypothesis(prediction_phd2, measurement)  # Group a prediction and measurement
    post_phd2 = updater_phd2.update(hypothesis_phd2)
    track_phd2.append(post_phd2)
    prior_phd2 = track_phd2[-1]
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-26-43d455073d56> in <module>()
          4     prediction_phd1 = predictor_phd1.predict(prior_phd1, timestamp=measurement.timestamp)
          5     hypothesis_phd1 = SingleHypothesis(prediction_phd1, measurement)  # Group a prediction and measurement
    ----> 6     post_phd1 = updater_phd1.update(hypothesis_phd1)
          7     track_phd1.append(post_phd1)
          8     prior_phd1 = track_phd1[-1]
    

    /usr/local/lib/python3.6/dist-packages/stonesoup/updater/pointprocess.py in update(self, hypotheses)
         57         weight_sum_list = list()
         58         # Loop over all measurements
    ---> 59         for multi_hypothesis in hypotheses[:-1]:
         60             updated_measurement_components = list()
         61             # Initialise weight sum for measurement to clutter intensity
    

    TypeError: 'SingleHypothesis' object is not subscriptable

