---
published: false
category: class
layout: default
---
# Memory #

Key words:

- **transparency** : The OS should implement virtual memory so it is invisible. Program shouldn't be aware of the virtualization.
- **efficiency** : make virtualization as efficient as possible. Both in time and space. For time-efficient virtualization, need hardware features such as TLB(to be explained).
- **protection** : OS should protect processes from one another. Each process should be **isolated** to prevent malicious programs.

The value from a pointer is always a **virtual address**.

```c
printf("location of main() : %p\n", (void *) main);
```

Gives:

> location of main() : 0x1095afe50

For these things to work, we need to learn the **mechanisms and policies**.

## Types of memory ##

- **stack** memory are managed *implicitly* by the compiler. This is also called **automatic** memory.
- **heap** memory is long lived. It's managed *explicitly* by programmers. Also harder for systems to create. 

We will show the two **library** (not system) calls that C uses. This is a library call because it makes some system calls within itself. It's a little more complicated than that. For example, **brk** gives the location of the end of the heap. We can increase or decrease the size of the heap. We can also get anonymous memory using **mmap** system call, which gives **swap space** memory.

### void \*malloc(size_t size) ###

- Pass in a size, and it either gives you new space or fails and returns NULL.
- To know the size, do not give a literal constant. Use the function **sizeof(type)**.
	- This is called a *compile-time* operator. It's known at compile time. They replace it with the actual literal value. sizeof() is NOT a function call. It's an OPERATOR!
- There's a similar call called **calloc()**. It zeroes the values out. 
- **realloc()** takes the memory of a memory, for e.x., and makes a larger region of memory and copies the old region into it. 
    
### void free() ###

Pretty self explanatory.

```c
int *x = malloc(10 * sizeof(int));
...
free(x); // no need to specify size :)
```

### Common errors ###

#### Pointers are tricky ####
```c
int *x = malloc(10 * sizeof(int));
// sizeof(x) gives us 4 bytes or 8 bytes.
// It's the SIZE OF THE POINTER. Not the array!
```

```c
int x[10];
// sizeof(x) gives us 40 bytes. This is
// What we wanted.
```

#### Strings are tricky ####

```c
char x[5] = "1234";
char *y = malloc(strlen(y) + 1); 
// remember the + 1!! We need the \0 character.
```

#### Forgetting to allocate ####

```c
char *dst;
strcpy(dst, src); // SEGFAULT!
```

```c
char *dst = (char *) malloc(strlen(src) + 1);
strcpy(dst, src); // yay :)
```

#### Not allocating enough memory (Buffer overflow) ####

```c
char *dst = (char *) malloc(strlen(src)); // too small!
strcpy(dst, src); // might work but...
// SECURITY RISK!
```

#### Forgetting to initialize allocated memory ####

```c
char *dst = (char *) malloc(strlen(src) + 1);
printf("%s", dst); 
// wait... It's printing garbage!
```

#### Forgetting to free memory ####

```c
while(true){
  char *dst = (char *) malloc(strlen(src) + 1);
  ...
  ...
}
// Our program slowly dies.
```

#### Freeing memory before done ####

```c
char *dst = (char *) malloc(strlen(src) + 1);
free(dst);
...
printf("%s", dst); 
// We already freed it! 
// If we malloc'd over that memory...
// Then we would be reading garbage. :(
```

#### Freeing memory repeatedly ####

```c
char *dst = (char *) malloc(strlen(src) + 1);
free(dst);
free(dst); // mem alloc library gets confused
// undefined behavior is scary...
```

#### Calling free() with incorrect value ####

Free expects us to pass in a legitimate pointer. If you pass in other things, **bad things happen**. Invalid frees are dangerous! Don't be a retard.~~

#### How to spot these problems? ####

Check out the tools **purify** and **valgrind**. They locate memory problems.

## Mechanism: Address Translation ##

- We use the idea **limited direct execution(LDE)**.
	- Most of the time let program run directly on hardware. At certain times arrange so OS gets involved.
- The technique is called **hardware-based address translation**. 
	- Hardware transform each memory access, changing each virtual address to a physical address.
    
### 3 Assumptions ###

1. Addr space is placed contiguously in physical memory.
2. The size of the virtual memory is less than the physical memory.
3. Each addr space is the same size.

### Dynamic (Hardware-based) Relocation ###

**base and bounds/dynamic relocation** : has a **base**, which is where the start of the address chunk is, and **bounds**, which is the size of the memory chunk.

```
physical address = virtual address + base
```

Say, for example, we start at address 32768 as the start of our program. It will look like this:

----
**stack** (eax pointing here. Line 500)

----
empty

----
**heap** (ebx pointing here. Line 350)

----
**code** (code pointing here. Line 128)

If our instruction was:

```
128: movl 0x0(%ebx), %eax
```

and assume ebx has the value 350, and %eax with the value 500.

Then really, it's actually the following:

```
128+32768: move the value at 350+32768 to 500+32768.
```

Now, what if eax was at line 1000, and our **bounds** in **base and bounds** is 800? Then it will throw an error(fault) and say that it's out of bounds.

There's actually a hardware component for this called **the memory management unit(MMU)**. When a context switch happens, we must *save and restore the base-and-bounds pair when switching between processes.* The data structure that is used to save the pair is called a **process control block**.

What happens is:

1. The OS initializes trap table, process table, etc. 
2. Hardware sets up illegal mem-access/instruction handler.
3. Hardware moves process to **user mode**.
4. Whenever process wants to access memory for code, variables, etc, direct translate via **base-and-bounds**.
5. If memory access illegal, hardware moves to **kernel mode** and goes to interrupt handler.
6. Now the OS intervenes. Decides to terminate and deallocate the memory from process. 

As you can see, the OS only does part 1 and 6. It waits patiently until the hardware jumps to interrupt/trap handler.

Cons of this approach:

- tons of space wasted. We have to allocate the same chunk of memory every time. The stack and the heap don't really grow much and so that chunk of memory is wasted. This is called **internal fragmentation**.

### Segmentation ###

Segmentation takes care of the question: **How do we support a large address space that might have a ton of wasted space in between the stack and the heap?**

The main idea is: instead of having 1 base-and-bounds pair in the MMU, why not have it per logical **segment** of addr space? Each logical segment is code, stack, and heap. Just put these in different places of the memory. *Segment these blocks of memory into 3 segments.*

**Note: This is why we call it "Segmentation fault", because it was a violation of memory access on a segmented machine to an illegal addr.**

In this, we can just make the heap its own segment, calling its first variables at address 0. 

We also need to take care of the stack. It grows downward which is a problem, so we need to support downward **base-and-bounds**. This is hardware-supported via a single bit. 0 for growing downwards and 1 for growing positive.

#### Support for code sharing ####

If we are calling the same routine over and over we don't want to load the same code. In this case we would want to make sections of code **read-execute only**. As long as we don't overwrite the memory, we are good. 

Now we need to create 2 classes: **read-execute only** and **read-write only**. The code segment is RE, and the heap and stack are RW. The bits that make these protection qualities are called **protection bits**.

#### Fine grained vs. Coarse grained ####

**Coarse grained** : When we only break up memory into large countable chunks. For example, we broke up the memory into stack, heap, and code. 3 Chunks is very coarse grained.

**Fine grained** : When we break up memory into a ton of small chunks. To implement this we need a **segment table**. 

#### Issues with segmentation ####

If we keep creating segments randomly on the main memory, they won't be perfectly fitting into the gaps. There will be little holes of memory in between 2 segments that is basically not usable. We call this **external fragmentation**.

How do we solve this problem? We have a couple options:

**First fit allocation**
- The first possible memory space that the process can use is used. 
- No matter how big or how small that available memory is, we give it to them. 
- When there are no more suitable chunks left, put large memory request access on hold. 
- Move to the next few bits of memory requests and give them the suitable available chunks. 
- When a large process is done and the memory is freed, we can then give the large request of memory on hold the memory that was just made available.

**Next fit allocation**
- Same as **first fit allocation** but starts the search for valid memory chunks AFTER the previous given chunk. 
- This way, we don't have to look back redundantly. When we get to the end, loop back around.

**Best fit allocation**
- Looks for the **best possible location** for a memory request. 
- Has a list of free spaces and returns the one most similar in size to the memory request.

**Slab allocation**
- Caches previously created free list objects. When destroyed, don't actually destroy them, but rather just wipe the references. 
- Each slab is a constant size, with smaller slabs to cover up the external fragmentation.

**Buddy allocation**
- Given a memory request of size n, give the smallest possible divided chunk that would accomodate that size.
- By giving the smallest possible, we divide the biggest chunk into 2 smaller chunks, then 1 of the smaller chunks into 2 even smaller chunks and so on until we find one.
- When two chunks of the same size that were once one bigger chunk are freed up, we merge the two chunks into a chunk of higher size.

# Paging #

**Paging** is chopping up address spaces into smaller units. Mapping is generally stored in physical memory, and so it's slow.

## How do we speed it up? ##

OS needs some help - needs hardware to make a **translation-lookaside buffer (TLB)**. TLB is a part of the **MMU**. It's a cache of virtual-to-physical address translations(so we should call it **address-translation cache**. This makes things a lot faster.

The basic algorithm is:

1. Extract virtual page number(VPN). Check if TLB holds this VPN.
2. If hit, then extract page frame number, concatenate to offset from original virtual address.
3. If not hit, hard ware access page table to find translation, then updates TLB.

Obviously, we want to reduce the # of misses as much as we can.

We basically want to reduce the cache misses. If we read in an array, every time the cache misses, we load that bit of memory into the cache so we can read the sequential bits easily without missing.

Two important terms: **Spatial locality** and **temporal locality**. **Spatial locality** is improved if we have big page sizes that can hold nearby values. **Temporal locality** is improved if the element stays within the cache and whenever it's re-referenced, is read easily from its page table.

## Who handles TLB misses? ##

In **CISC**, hardware handles TLB miss completely. In **RISC**, hardware raises exception, pauses instruction stream, goes to kernel mode, into a trap handler, and then OS handles TLB misses by updating TLB.

This trap handler, however, is slightly different. Trap resume execution at instruction **AFTER** trap into OS, but in our TLB case, it re-executes the instruction that **CAUSED** the trap. Therefore, it redoes the instruction.

## What is inside a TLB ##

TLB's are **fully associative**, which means any given translation could be anywhere in TLB. TLB has the Virtual Page Number, a Physical Frame Number(the physical version of VPN), and other bits.

In the other bits, there's a protection bit(can we access this memory), a valid bit(whether address is cached), and an **address space identifier(ASID)**(kind of like a PID, but smaller range. Used to distinguish which process is using this VPN). 

## How to replace cache when it's filled? ##

Sometimes people use **least-recently-used(LRU)**, but it's really bad for when you have n+1 pages with TLB of size n. Imagine it - you'd be trying to find the least-recently-used address everytime, but right before you get it it's removed from the TLB. For this purpose, some like to just use random policy.

## Real TLB ##

Case study of MIPS R4000:
1. 19 bits for VPN
2. global bit(whether page is globally shared. When it's set, the ASID is ignored)
3. coherence bit(how page is cached)
4. dirty bit(whether page is written to)
5. valid bit(whether there is a translation)
6. page mask(supports multiple page sizes)

## Problems ##

If program uses large number of pages(more than TLB can have), then it's called **exceeding the TLB coverage**. We can fix this by supporting larger sized pages, which is used in DMBS.

# Hard Disk Memory #

We want the memory-space for any application to be large enough that we don't have to worry about **memory overlays**, which needs programmers to move data in and out of memory. We can assume everything is inside the memory.

## Swap Space ##

There's some memory on the disk called **swap space** because swap pages out of memory to it and vice versa. Assume OS can read/write to swap space, which has its own disk address. Also assume swap space is very large.

If we want to swap pages in and out of disk, then we need to give it some extra information about whether it's in or out of disk or not. In this case, we give it a **present bit**, 1 for in memory, and 0 for on disk. Accessing a page that's not in memory is called a **page fault**, and we need a **page fault handler** to take care of the situation.

When there is a page fault, TLB gets updated and the present bit gets swapped from the page on memory that's going into disk and the on-disk page that was requested via page fault handler. This I/O is very expensive. During this time, the process is in a **blocked state**, which means other processes use cpu and memory during this time.

## We don't always only replace/read disk when the memory is full ##

Many times we don't want the memory to be 100% full. OS needs some of it to do some stuff.

OS has some kind of **high watermark(HW)** and **low watermark(LW)** to decide when to evict pages. When there are less than LW pages, then it will start freeing memory into disk until it has reached the HW number of pages. This is done via a background thread called **swap daemon or page daemon**. Some systems cluster pages to be written all out into disk at once.

# Paging policies #

The metric we use is **AMAT - average memory access time**. It's calculated like the following:

AMAT = (P_hit \* T_m) + (P_miss \* T_d)

In the equation, T_m is cost of accessing memory, T_d is cost of accessing disk. P is the probability, and so P_hit + P_miss = 1. In modern times, T_d is on orders thousands times faster than T_m, so we need to just reduce T_d to get overall greater performance.

The perfect policy is one that **replaces the page that will be accessed furthest in the future**. We will compare our policy to this.

## FIFO ##

FIFO is implemented using a queue. First in gets evicted first when we have to. It's pretty bad.

## Random ##

Random picks a random page to replace. Its effectiveness is pretty much only based on the luck of the draw. About 40% of the time it does as good as optimal, but other times it's anomalously bad.

## LRU ##

LRU picks the least recently used page to evict. It does pretty well. Better than the random/FIFO at least, because it uses history to make more informed decisions to how often a page is used(aka how important it is to keep the page in the TLB).

## How to gauge these policies? ##

We use a workload situation called the "80-20" workload, which exhibits locality. **80% of references are made to 20% of the pages**. LRU Would do a lot better in this case because the 20% of the pages are never really removed.

If we use a workload situation called the looping workload where we have the N+1 workload of pages with cache size of 49, we would have a 0% hit rate on both FIFO and LRU. However, random would do alright.

## Approximating LRU ##

The **clock algorithm** is of the following: imagine all pages are in a circular list. Go through them and if the current page has a use bit of 0, then we know it hasn't been recently used - evict it! Otherwise, it's 1, and we set it to 0, then move on to the next page.

## Dirty Pages ##

If a page is dirty, it means that **it has been modified on memory, but not on disk.** Therefore, we need to write it back to update it. Eviction is expensive for these. Clean pages are free in terms of eviction.

## Page selection ##

On demand, the page chosen is going to be brought into memory, but the pages next to it might also be brought into memory in advance for speed. This behavior is called **clustering**.

## Thrashing ##

**Thrashing** is when memory demands is too high, then the system will be constantly paging. Sometimes, systems say to halt everything and hope that the memory requests decrease. Othertimes, like some versions of Linux, the **out of memory killer** kills the process that uses a ton of memory. Could lead to weird behavior though.

<script src="https://utteranc.es/client.js" repo="OneRaynyDay/oneraynyday.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script>
